{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Membros do grupo:\n",
    "\n",
    "Alonso Batista de Oliveira Júnior\n",
    "André Moreira de Carvalho\n",
    "Gustavo Castro Candeia\n",
    "Halex Maciel Silva Vieira\n",
    "Welbert Luiz Silva Junior\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, cohen_kappa_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import joblib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de constantes para o caminho dos arquivos\n",
    "DATA_FILE_PATH = '../data/cleaned_dataset.csv'\n",
    "MODEL_DIR = '../models'\n",
    "LOG_DIR = '../logs'\n",
    "\n",
    "# Save the log to a file in the logs directory\n",
    "logging.basicConfig(filename=f'{LOG_DIR}/training.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "MODEL_CONFIGS: Dict[str, Dict[str, Any]] = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'clf__n_estimators': [100, 300, 500],\n",
    "            'clf__max_depth': [10, 30, 50],\n",
    "            'clf__min_samples_split': [2, 5, 10],\n",
    "            'clf__min_samples_leaf': [1, 2, 4],\n",
    "            'clf__max_features': ['auto', 'sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'clf__n_estimators': [100, 300, 500],\n",
    "            'clf__learning_rate': [0.01, 0.1, 0.001],\n",
    "            'clf__max_depth': [3, 5, 7],\n",
    "            'clf__subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(filepath, usecols=['Clean_Text_LSTM', 'Label'])\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Arquivo não encontrado: {filepath}\")\n",
    "        raise e\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, LabelEncoder]:\n",
    "    df['Clean_Text_LSTM'] = df['Clean_Text_LSTM'].fillna('').astype(str)\n",
    "    texts = df['Clean_Text_LSTM'].apply(lambda x: x.split()).values\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    labels = encoder.fit_transform(df['Label'])\n",
    "    \n",
    "    # Imprime os labels e seus códigos\n",
    "    print(\"Classes codificadas e seus códigos:\")\n",
    "    for label, code in zip(encoder.classes_, range(len(encoder.classes_))):\n",
    "        print(f\"Label '{label}' é codificado como {code}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    return train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels), encoder\n",
    "\n",
    "def train_word2vec(texts: List[List[str]], vector_size: int = 300, window: int = 5, min_count: int = 2) -> Word2Vec:\n",
    "    model = Word2Vec(sentences=texts, vector_size=vector_size, window=window, min_count=min_count, workers=4)\n",
    "    return model\n",
    "\n",
    "def text_to_word2vec(texts: List[List[str]], model: Word2Vec) -> np.ndarray:\n",
    "    def get_vector(text):\n",
    "        vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.array([get_vector(text) for text in texts])\n",
    "\n",
    "def create_pipeline(model: Any) -> Pipeline:\n",
    "    return Pipeline([\n",
    "        ('clf', model)\n",
    "    ])\n",
    "\n",
    "def additional_metrics(y_test: np.ndarray, predicted_probabilities: np.ndarray, predictions: np.ndarray) -> Dict[str, Any]:\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "    kappa = cohen_kappa_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predicted_probabilities, multi_class='ovr', average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "        \"f1_score\": f1,\n",
    "        \"kappa_score\": kappa,\n",
    "        \"roc_auc_per_class\": roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: Pipeline, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:\n",
    "    try:\n",
    "        predictions = model.predict(X_test)\n",
    "        predicted_probabilities = model.predict_proba(X_test)\n",
    "\n",
    "        base_metrics = {\n",
    "            'classification_report': classification_report(y_test, predictions, output_dict=True),\n",
    "            'accuracy': accuracy_score(y_test, predictions),\n",
    "            'roc_auc': roc_auc_score(y_test, predicted_probabilities, multi_class='ovr', average='weighted')\n",
    "        }\n",
    "\n",
    "        add_metrics = additional_metrics(y_test, predicted_probabilities, predictions)\n",
    "        base_metrics.update(add_metrics)\n",
    "        return base_metrics\n",
    "    except NotFittedError as e:\n",
    "        logging.error(\"Modelo não ajustado\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao avaliar o modelo: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_performance(cv_results, model_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cv_results['mean_test_score'], label='Média da Pontuação de Teste')\n",
    "    plt.plot(cv_results['mean_train_score'], label='Média da Pontuação de Treino')\n",
    "    plt.fill_between(range(len(cv_results['mean_test_score'])), \n",
    "                     cv_results['mean_test_score'] - cv_results['std_test_score'], \n",
    "                     cv_results['mean_test_score'] + cv_results['std_test_score'], alpha=0.2)\n",
    "    plt.fill_between(range(len(cv_results['mean_train_score'])), \n",
    "                     cv_results['mean_train_score'] - cv_results['std_train_score'], \n",
    "                     cv_results['mean_train_score'] + cv_results['std_train_score'], alpha=0.2)\n",
    "    plt.title(f'Desempenho do Treinamento do {model_name}')\n",
    "    plt.xlabel('Índice de Combinação de Hiperparâmetros')\n",
    "    plt.ylabel('Pontuação')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, classes):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    df = load_data(DATA_FILE_PATH)\n",
    "    (X_train, X_test, y_train, y_test), label_encoder = prepare_data(df)\n",
    "    joblib.dump(label_encoder, f'{MODEL_DIR}/label_encoder.joblib')\n",
    "    \n",
    "    word2vec_model = train_word2vec(X_train)\n",
    "    X_train_w2v = text_to_word2vec(X_train, word2vec_model)\n",
    "    X_test_w2v = text_to_word2vec(X_test, word2vec_model)\n",
    "    \n",
    "    model_performances = []\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, config in tqdm(MODEL_CONFIGS.items(), desc=\"Treinando modelos\"):\n",
    "        pipeline = create_pipeline(config['model'])\n",
    "        search = RandomizedSearchCV(pipeline, config['params'], cv=kfold, scoring='accuracy', n_jobs=-1, verbose=2, return_train_score=True)\n",
    "        search.fit(X_train_w2v, y_train)\n",
    "        \n",
    "        # Plot training performance\n",
    "        plot_training_performance(search.cv_results_, name)\n",
    "        \n",
    "        metrics = evaluate_model(search.best_estimator_, X_test_w2v, y_test)\n",
    "        logging.info(f\"Relatório de classificação para {name}:\\n{metrics['classification_report']}\")\n",
    "        model_performances.append((name, metrics['accuracy'], metrics['roc_auc'], search.best_estimator_))\n",
    "        logging.info(f\"Melhores hiperparâmetros para {name}: {search.best_params_}\")\n",
    "        logging.info(f\"Acurácia para {name}: {metrics['accuracy']:.4f}\")\n",
    "        logging.info(f\"ROC-AUC para {name}: {metrics['roc_auc']:.4f}\")\n",
    "        logging.info(f\"Melhor modelo para {name}: {search.best_estimator_}\\n\")\n",
    "\n",
    "    model_performances.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (name, acc, auc, model) in enumerate(model_performances, 1):\n",
    "        model_path = f'{MODEL_DIR}/top_{i}_{name}_model.joblib'\n",
    "        joblib.dump(model, model_path)\n",
    "        logging.info(f\"Modelo {name} salvo com acurácia de {acc:.4f} em {model_path}\")\n",
    "\n",
    "    # Plot confusion matrix for the best model\n",
    "    best_model_name, _, _, best_model = model_performances[0]\n",
    "    best_predictions = best_model.predict(X_test_w2v)\n",
    "    best_conf_matrix = confusion_matrix(y_test, best_predictions)\n",
    "    plot_confusion_matrix(best_conf_matrix, label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
